{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 16\n",
    "# Improve Performance with Algorithm Tuning\n",
    "\n",
    "Machine learning <a>models are parameterized so that their behavior can be tuned for a given problem. Models can have many parameters and finding the best combination of parameters can be treated as a search problem</a>. In this chapter you will discover how to tune the parameters of machine learning algorithms in Python using the scikit-learn. After completing this lesson you will know:\n",
    "1. The importance of algorithm parameter tuning to improve algorithm performance.\n",
    "2. How to use a grid search algorithm tuning strategy.\n",
    "3. How to use a random search algorithm tuning strategy.\n",
    "\n",
    "Let's get started.\n",
    "\n",
    "## 16.1 Machine Learning Algorithm Parameters\n",
    "\n",
    "Algorithm tuning is a final step in the process of applied machine learning before finalizing your model. It is sometimes called hyperparameter optimization where the algorithm parameters are referred to as hyperparameters, whereas the coeficients found by the machine learning algorithm itself are referred to as parameters. Optimization suggests the search-nature of the problem. Phrased as a search problem, you can use different search strategies to find a good and robust parameter or set of parameters for an algorithm on a given problem. Python scikit-learn provides two simple methods for algorithm parameter tuning:\n",
    "- Grid Search Parameter Tuning.\n",
    "- Random Search Parameter Tuning.\n",
    "\n",
    "## 16.2 Grid Search Parameter Tuning\n",
    "\n",
    "`Grid search` is an <a>approach to parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters speciffied in a grid</a>. You can perform a grid search using the GridSearchCV class. The example below evaluates different alpha values for the `Ridge Regression` algorithm on the dataset. This is a one-dimensional grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2796175593129722\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Grid Search for Algorithm Tuning\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# load data\n",
    "filename = '.\\data\\pima-indians-diabetes.data.csv'\n",
    "\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "alphas = numpy.array([1,0.1,0.01,0.001,0.0001,0])\n",
    "param_grid = dict(alpha=alphas)\n",
    "\n",
    "model = Ridge()\n",
    "grid = GridSearchCV(cv=3, estimator=model, param_grid=param_grid)\n",
    "grid.fit(X, Y)\n",
    "\n",
    "print(grid.best_score_)\n",
    "\n",
    "print(grid.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example lists out the optimal score achieved and the set of parameters in the grid that achieved that score. In this case the alpha value of 1.0.\n",
    "\n",
    "## 16.3 Random Search Parameter Tuning\n",
    "\n",
    "`Random` search is an <a>approach to parameter tuning that will sample algorithm parameters from a random distribution (i.e. uniform) for a fixed number of iterations. A model is constructed and evaluated for each combination of parameters chosen</a>. You can perform a random search for algorithm parameters using the RandomizedSearchCV class. The example below evaluates different random alpha values between 0 and 1 for the **Ridge Regression** algorithm on the dataset. A total of 100 iterations are performed with uniformly random alpha values selected in the range between 0 and 1 (the range that alpha values can take)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27961712703051084\n",
      "0.9779895119966027\n"
     ]
    }
   ],
   "source": [
    "#Randomized for Algorithm Tuning\n",
    "import numpy\n",
    "from pandas import read_csv\n",
    "from scipy.stats import uniform\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# load data\n",
    "filename = '.\\data\\pima-indians-diabetes.data.csv'\n",
    "\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(filename, names=names)\n",
    "\n",
    "array = dataframe.values\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "param_grid = {'alpha': uniform()}\n",
    "model = Ridge()\n",
    "rsearch = RandomizedSearchCV(cv=3, estimator=model, param_distributions=param_grid, n_iter=100,\n",
    "random_state=7)\n",
    "rsearch.fit(X, Y)\n",
    "\n",
    "print(rsearch.best_score_)\n",
    "\n",
    "print(rsearch.best_estimator_.alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the example produces results much like those in the grid search example above. An optimal alpha value near 1.0 is discovered."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
